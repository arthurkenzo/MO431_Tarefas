{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 16:01:12.576026: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1732993272.635521  145704 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1732993272.649805  145704 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 16:01:12.775407: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-30 16:01:16.470718: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.4346\n",
      "Epoch 2, Loss: 0.3632\n",
      "Epoch 3, Loss: 0.3944\n",
      "Epoch 4, Loss: 0.3344\n",
      "Epoch 5, Loss: 0.3177\n",
      "Epoch 6, Loss: 0.3213\n",
      "Epoch 7, Loss: 0.4139\n",
      "Epoch 8, Loss: 0.3866\n",
      "Epoch 9, Loss: 0.2597\n",
      "Epoch 10, Loss: 0.2056\n",
      "Epoch 11, Loss: 0.3927\n",
      "Epoch 12, Loss: 0.3028\n",
      "Epoch 13, Loss: 0.2031\n",
      "Epoch 14, Loss: 0.2321\n",
      "Epoch 15, Loss: 0.3409\n",
      "Epoch 16, Loss: 0.2421\n",
      "Epoch 17, Loss: 0.2282\n",
      "Epoch 18, Loss: 0.1438\n",
      "Epoch 19, Loss: 0.2458\n",
      "Epoch 20, Loss: 0.4502\n",
      "Epoch 21, Loss: 0.3665\n",
      "Epoch 22, Loss: 0.2635\n",
      "Epoch 23, Loss: 0.2001\n",
      "Epoch 24, Loss: 0.4373\n",
      "Epoch 25, Loss: 0.2229\n",
      "Epoch 26, Loss: 0.4342\n",
      "Epoch 27, Loss: 0.3146\n",
      "Epoch 28, Loss: 0.0944\n",
      "Epoch 29, Loss: 0.1853\n",
      "Epoch 30, Loss: 0.2379\n",
      "Epoch 31, Loss: 0.2794\n",
      "Epoch 32, Loss: 0.4180\n",
      "Epoch 33, Loss: 0.1355\n",
      "Epoch 34, Loss: 0.2701\n",
      "Epoch 35, Loss: 0.1833\n",
      "Epoch 36, Loss: 0.2290\n",
      "Epoch 37, Loss: 0.1999\n",
      "Epoch 38, Loss: 0.2637\n",
      "Epoch 39, Loss: 0.2101\n",
      "Epoch 40, Loss: 0.3506\n",
      "Epoch 41, Loss: 0.2325\n",
      "Epoch 42, Loss: 0.1742\n",
      "Epoch 43, Loss: 0.2879\n",
      "Epoch 44, Loss: 0.1765\n",
      "Epoch 45, Loss: 0.3665\n",
      "Epoch 46, Loss: 0.0943\n",
      "Epoch 47, Loss: 0.1538\n",
      "Epoch 48, Loss: 0.2640\n",
      "Epoch 49, Loss: 0.2617\n",
      "Epoch 50, Loss: 0.1655\n",
      "Test Accuracy: 92.46%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import numpy as np\n",
    "\n",
    "# Load and preprocess the MNIST data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train.reshape(-1, 28 * 28).astype('float32') / 255.0  # Flatten and normalize\n",
    "x_test = x_test.reshape(-1, 28 * 28).astype('float32') / 255.0\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)  # One-hot encode labels\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Define model parameters\n",
    "num_features = 28 * 28  # Input size (flattened image)\n",
    "num_classes = 10        # Number of classes\n",
    "learning_rate = 0.1     # Gradient descent learning rate\n",
    "num_epochs = 50         # Number of training epochs\n",
    "batch_size = 128\n",
    "\n",
    "# Define weights and bias\n",
    "weights = tf.Variable(tf.random.normal([num_features, num_classes], stddev=0.01))\n",
    "bias = tf.Variable(tf.zeros([num_classes]))\n",
    "\n",
    "# Define the softmax regression model\n",
    "def multinomialLogisticRegression(x):\n",
    "    logits = tf.matmul(x, weights) + bias\n",
    "    return tf.nn.softmax(logits), logits\n",
    "\n",
    "# Define cross-entropy loss\n",
    "def computeLoss(logits, labels):\n",
    "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "\n",
    "# Define the gradient descent optimizer\n",
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def trainStep(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        _, logits = multinomialLogisticRegression(x)\n",
    "        loss = computeLoss(logits, y)\n",
    "    gradients = tape.gradient(loss, [weights, bias])\n",
    "    optimizer.apply_gradients(zip(gradients, [weights, bias]))\n",
    "    return loss\n",
    "\n",
    "# Training process\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle data\n",
    "    indices = np.random.permutation(x_train.shape[0])\n",
    "    x_train, y_train = x_train[indices], y_train[indices]\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for i in range(0, x_train.shape[0], batch_size):\n",
    "        x_batch = x_train[i:i + batch_size]\n",
    "        y_batch = y_train[i:i + batch_size]\n",
    "        loss = trainStep(x_batch, y_batch)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# Testing\n",
    "_, logits = multinomialLogisticRegression(x_test)\n",
    "predictions = tf.argmax(logits, axis=1)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, tf.argmax(y_test, axis=1)), tf.float32))\n",
    "print(f\"Test Accuracy: {accuracy.numpy() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Loss = 4.3034, Gradient Norm = 2.4392\n",
      "Iter 2: Loss = 5.4886, Gradient Norm = 2.0397\n",
      "Iter 3: Loss = 5.5762, Gradient Norm = 2.4415\n",
      "Iter 4: Loss = 6.1143, Gradient Norm = 1.9909\n",
      "Iter 5: Loss = 4.7195, Gradient Norm = 1.6399\n",
      "Iter 6: Loss = 3.7657, Gradient Norm = 0.8118\n",
      "Iter 7: Loss = 3.5128, Gradient Norm = 0.7137\n",
      "Iter 8: Loss = 3.2963, Gradient Norm = 0.7274\n",
      "Iter 9: Loss = 3.1387, Gradient Norm = 0.7146\n",
      "Iter 10: Loss = 2.8514, Gradient Norm = 0.5666\n",
      "Iter 11: Loss = 2.6831, Gradient Norm = 0.5078\n",
      "Iter 12: Loss = 2.5576, Gradient Norm = 0.5127\n",
      "Iter 13: Loss = 2.4300, Gradient Norm = 0.4369\n",
      "Iter 14: Loss = 2.3100, Gradient Norm = 0.4337\n",
      "Iter 15: Loss = 2.2064, Gradient Norm = 0.3634\n",
      "Iter 16: Loss = 2.0934, Gradient Norm = 0.3381\n",
      "Iter 17: Loss = 2.0188, Gradient Norm = 0.3088\n",
      "Iter 18: Loss = 1.9339, Gradient Norm = 0.3008\n",
      "Iter 19: Loss = 1.8489, Gradient Norm = 0.2960\n",
      "Iter 20: Loss = 1.7939, Gradient Norm = 0.2677\n",
      "Iter 21: Loss = 1.7433, Gradient Norm = 0.2570\n",
      "Iter 22: Loss = 1.7027, Gradient Norm = 0.2736\n",
      "Iter 23: Loss = 1.6553, Gradient Norm = 0.2335\n",
      "Iter 24: Loss = 1.6119, Gradient Norm = 0.2175\n",
      "Iter 25: Loss = 1.5768, Gradient Norm = 0.2134\n",
      "Iter 26: Loss = 1.5337, Gradient Norm = 0.2036\n",
      "Iter 27: Loss = 1.4928, Gradient Norm = 0.1895\n",
      "Iter 28: Loss = 1.4601, Gradient Norm = 0.1871\n",
      "Iter 29: Loss = 1.4215, Gradient Norm = 0.1741\n",
      "Iter 30: Loss = 1.4026, Gradient Norm = 0.1725\n",
      "Iter 31: Loss = 1.3721, Gradient Norm = 0.1667\n",
      "Iter 32: Loss = 1.3518, Gradient Norm = 0.1691\n",
      "Iter 33: Loss = 1.3292, Gradient Norm = 0.1531\n",
      "Iter 34: Loss = 1.3084, Gradient Norm = 0.1507\n",
      "Iter 35: Loss = 1.2894, Gradient Norm = 0.1502\n",
      "Iter 36: Loss = 1.2733, Gradient Norm = 0.1561\n",
      "Iter 37: Loss = 1.2592, Gradient Norm = 0.1400\n",
      "Iter 38: Loss = 1.2459, Gradient Norm = 0.1425\n",
      "Iter 39: Loss = 1.2295, Gradient Norm = 0.1356\n",
      "Iter 40: Loss = 1.2085, Gradient Norm = 0.1290\n",
      "Iter 41: Loss = 1.1954, Gradient Norm = 0.1364\n",
      "Iter 42: Loss = 1.1795, Gradient Norm = 0.1246\n",
      "Iter 43: Loss = 1.1639, Gradient Norm = 0.1204\n",
      "Iter 44: Loss = 1.1486, Gradient Norm = 0.1195\n",
      "Iter 45: Loss = 1.1370, Gradient Norm = 0.1191\n",
      "Iter 46: Loss = 1.1258, Gradient Norm = 0.1160\n",
      "Iter 47: Loss = 1.1134, Gradient Norm = 0.1205\n",
      "Iter 48: Loss = 1.1032, Gradient Norm = 0.1153\n",
      "Iter 49: Loss = 1.0886, Gradient Norm = 0.1068\n",
      "Iter 50: Loss = 1.0764, Gradient Norm = 0.1057\n",
      "Iter 51: Loss = 1.0665, Gradient Norm = 0.1030\n",
      "Iter 52: Loss = 1.0543, Gradient Norm = 0.1038\n",
      "Iter 53: Loss = 1.0461, Gradient Norm = 0.1016\n",
      "Iter 54: Loss = 1.0366, Gradient Norm = 0.0994\n",
      "Iter 55: Loss = 1.0304, Gradient Norm = 0.0979\n",
      "Iter 56: Loss = 1.0237, Gradient Norm = 0.0956\n",
      "Iter 57: Loss = 1.0151, Gradient Norm = 0.0984\n",
      "Iter 58: Loss = 1.0072, Gradient Norm = 0.0946\n",
      "Iter 59: Loss = 0.9986, Gradient Norm = 0.0926\n",
      "Iter 60: Loss = 0.9937, Gradient Norm = 0.0902\n",
      "Iter 61: Loss = 0.9850, Gradient Norm = 0.0895\n",
      "Iter 62: Loss = 0.9761, Gradient Norm = 0.0874\n",
      "Iter 63: Loss = 0.9700, Gradient Norm = 0.0872\n",
      "Iter 64: Loss = 0.9611, Gradient Norm = 0.0856\n",
      "Iter 65: Loss = 0.9544, Gradient Norm = 0.0840\n",
      "Iter 66: Loss = 0.9479, Gradient Norm = 0.0809\n",
      "Iter 67: Loss = 0.9420, Gradient Norm = 0.0812\n",
      "Iter 68: Loss = 0.9373, Gradient Norm = 0.0853\n",
      "Iter 69: Loss = 0.9313, Gradient Norm = 0.0814\n",
      "Iter 70: Loss = 0.9262, Gradient Norm = 0.0781\n",
      "Iter 71: Loss = 0.9207, Gradient Norm = 0.0797\n",
      "Iter 72: Loss = 0.9163, Gradient Norm = 0.0788\n",
      "Iter 73: Loss = 0.9120, Gradient Norm = 0.0767\n",
      "Iter 74: Loss = 0.9054, Gradient Norm = 0.0745\n",
      "Iter 75: Loss = 0.9004, Gradient Norm = 0.0751\n",
      "Iter 76: Loss = 0.8955, Gradient Norm = 0.0728\n",
      "Iter 77: Loss = 0.8910, Gradient Norm = 0.0766\n",
      "Iter 78: Loss = 0.8846, Gradient Norm = 0.0714\n",
      "Iter 79: Loss = 0.8809, Gradient Norm = 0.0703\n",
      "Iter 80: Loss = 0.8764, Gradient Norm = 0.0724\n",
      "Iter 81: Loss = 0.8714, Gradient Norm = 0.0702\n",
      "Iter 82: Loss = 0.8677, Gradient Norm = 0.0749\n",
      "Iter 83: Loss = 0.8630, Gradient Norm = 0.0697\n",
      "Iter 84: Loss = 0.8597, Gradient Norm = 0.0683\n",
      "Iter 85: Loss = 0.8571, Gradient Norm = 0.0742\n",
      "Iter 86: Loss = 0.8531, Gradient Norm = 0.0688\n",
      "Iter 87: Loss = 0.8490, Gradient Norm = 0.0675\n",
      "Iter 88: Loss = 0.8454, Gradient Norm = 0.0673\n",
      "Iter 89: Loss = 0.8422, Gradient Norm = 0.0637\n",
      "Iter 90: Loss = 0.8378, Gradient Norm = 0.0667\n",
      "Iter 91: Loss = 0.8339, Gradient Norm = 0.0627\n",
      "Iter 92: Loss = 0.8306, Gradient Norm = 0.0650\n",
      "Iter 93: Loss = 0.8276, Gradient Norm = 0.0643\n",
      "Iter 94: Loss = 0.8248, Gradient Norm = 0.0630\n",
      "Iter 95: Loss = 0.8218, Gradient Norm = 0.0632\n",
      "Iter 96: Loss = 0.8188, Gradient Norm = 0.0632\n",
      "Iter 97: Loss = 0.8160, Gradient Norm = 0.0667\n",
      "Iter 98: Loss = 0.8127, Gradient Norm = 0.0606\n",
      "Iter 99: Loss = 0.8095, Gradient Norm = 0.0596\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Reshape and normalize pixel values\n",
    "x_train = x_train.reshape(x_train.shape[0], -1).astype(np.float32) / 255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1).astype(np.float32) / 255.0\n",
    "\n",
    "# One-hot encoding for labels\n",
    "y_train = np.eye(10, dtype=int)[y_train]\n",
    "y_test = np.eye(10, dtype=int)[y_test]\n",
    "\n",
    "# Define helper functions\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Stability improvement\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "def lossFunction(x, y):\n",
    "    s = softmax(x)\n",
    "    return -np.sum(y * np.log(s)) / x.shape[0]  # Average loss\n",
    "\n",
    "def gradient(x, wTx, y):\n",
    "    s = softmax(wTx)\n",
    "    return (x.T @ (s - y)) / x.shape[0]\n",
    "\n",
    "# Initialize parameters\n",
    "K = y_train.shape[1]\n",
    "n = x_train.shape[1]\n",
    "w = np.random.random_sample((n, K))\n",
    "\n",
    "# Training hyperparameters\n",
    "gradientNormStop = 1e-3\n",
    "maxSteps = 100\n",
    "learningRate = 1\n",
    "stochasticity = 0.1\n",
    "\n",
    "iter = 1\n",
    "gradientNorm = np.inf\n",
    "\n",
    "# Training loop\n",
    "while iter < maxSteps and gradientNorm > gradientNormStop:\n",
    "    \n",
    "    wTx = x_train @ w\n",
    "    grad = gradient(x_train, wTx, y_train)\n",
    "    gradientNorm = np.linalg.norm(grad)\n",
    "    w -= learningRate * (grad + stochasticity * gradientNorm * np.random.random_sample(grad.shape))  # Update weights\n",
    "    loss = lossFunction(wTx, y_train)\n",
    "\n",
    "    print(f\"Iter {iter}: Loss = {loss:.4f}, Gradient Norm = {gradientNorm:.4f}\")\n",
    "    iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 81.97%\n"
     ]
    }
   ],
   "source": [
    "def predict(x, w):\n",
    "    probabilities = softmax(x @ w)\n",
    "    return np.argmax(probabilities, axis=1)  # Return class with highest probability\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "    return np.mean(predictions == labels) * 100\n",
    "\n",
    "# Test the accuracy\n",
    "test_predictions = predict(x_test, w)\n",
    "true_labels = np.argmax(y_test, axis=1)  # Convert one-hot encoding back to class labels\n",
    "test_accuracy = accuracy(test_predictions, true_labels)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.66666667, 1.        ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "\n",
    "a/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mo431",
   "language": "python",
   "name": "mo431"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
